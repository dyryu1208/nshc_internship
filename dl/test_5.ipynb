{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport re\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras.callbacks import *\nfrom nltk.corpus import stopwords\nimport warnings\nwarnings.filterwarnings('ignore')\nnltk.download('words')\nnltk.download('stopwords')\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:59:57.489182Z","iopub.execute_input":"2021-12-09T03:59:57.48972Z","iopub.status.idle":"2021-12-09T04:00:03.877113Z","shell.execute_reply.started":"2021-12-09T03:59:57.48968Z","shell.execute_reply":"2021-12-09T04:00:03.876383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 전처리\n#### 전처리 실행한 모델과\n#### 아무것도 전처리하지 않은 모델 간 성능비교할 것","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('eda_data.csv')\nprint(train.head())\ninput = train['html']\ntarget = train['label']","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:00:03.878476Z","iopub.execute_input":"2021-12-09T04:00:03.878893Z","iopub.status.idle":"2021-12-09T04:00:05.171242Z","shell.execute_reply.started":"2021-12-09T04:00:03.878855Z","shell.execute_reply":"2021-12-09T04:00:05.170542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# bert_layer : Bert 모델 층 로드\n* 로드할 기본 Bert모델도 성능이 좋아\n* 추가로 많은 층을 쌓을 필요가 없음\n# vocab_file : Bert 모델 내 단어 로드\n* Bert모델은 우리 데이터셋 내 단어가 자신들의 단어사전에 포함되어 있는지 확인하고\n    * 없다면 그 단어를 하위 단어로 분할해서 확인하는 과정을 반복한다\n    * 최종적으로는 알파벳 단위로까지 분할해서 학습 --> 은어들도 학습은 가능","metadata":{}},{"cell_type":"code","source":"bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\",\n                           trainable=True)   \nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()  ","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:00:05.173207Z","iopub.execute_input":"2021-12-09T04:00:05.173475Z","iopub.status.idle":"2021-12-09T04:00:36.042149Z","shell.execute_reply.started":"2021-12-09T04:00:05.173439Z","shell.execute_reply":"2021-12-09T04:00:36.041441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tensorflow의 Bert모델 관련 모듈 다운로드","metadata":{}},{"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:00:36.043272Z","iopub.execute_input":"2021-12-09T04:00:36.045288Z","iopub.status.idle":"2021-12-09T04:00:36.944865Z","shell.execute_reply.started":"2021-12-09T04:00:36.045239Z","shell.execute_reply":"2021-12-09T04:00:36.94397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 데이터 전처리 함수\nimport tokenization\ntk = tokenization.FullTokenizer(vocab_file)   # Bert용 토크나이저 --> 단어를 음절단위로 구분\n\ndef bert_encode(html,tk,max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    for text in html:\n        text = tk.tokenize(text)   # 토크나이징\n        text = text[:max_len-2]  \n        # 단어 맨 뒤 2개 안가져오는 이유? 빈칸을 만들어놓고 special token을 넣기 위함\n        # special token을 통해 어떤 학습을 할지 Bert 모델이 결정함\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]    # 문장 앞뒤로 special token을 넣어줌\n        # cls --> 분류모델용 special token\n        # sep --> 문장간 구분자 역할\n        pad_len = max_len-len(input_sequence)  # max_len 보다 html 짧은 경우 남는 공간      \n        tokens = tk.convert_tokens_to_ids(input_sequence)    # 각 단어를 숫자 임베딩\n        tokens += [0] * pad_len      # pad_len은 0을 부여할 것\n        all_tokens.append(tokens)    # all_tokens에는 전체 html이 존재\n        \n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        # mask : 모델에게 어디서부터 어디까지 봐야하는지 알려주는 기능\n        # 어디서부터 어디까지가 실제 문장이고 의미없는 패딩인지 구분하는 기능\n        all_masks.append(pad_masks)\n        \n        segment_ids = [0] * max_len      \n        # 각 단어가 어떤 문장에 포함되어 있는지 모델에게 알려주는 기능\n        # 보통 단일문장은[1], 다중문장은[0] 값을 부여한다고 함\n        all_segments.append(segment_ids)\n        \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n    # array형태로 바꿔주어야 학습가능\n\ntrain_input = bert_encode(input.values,tk,max_len=110)  # max_len은 변경가능!","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:00:36.947494Z","iopub.execute_input":"2021-12-09T04:00:36.948063Z","iopub.status.idle":"2021-12-09T04:04:48.528084Z","shell.execute_reply.started":"2021-12-09T04:00:36.947912Z","shell.execute_reply":"2021-12-09T04:04:48.527216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 결과물\n* all_tokens = [[CLS], 80, 90, 222.......[SEP], [CLS], 20, 80, 1110, ....[SEP], .......]\n* all_masks = [1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,....]\n* all_segments = [0,0,0,0,0,0,0,.....0,0,0,0]","metadata":{}},{"cell_type":"code","source":"# 모델링(input 요소가 3개이므로 input층도 3개)\n\ndef build_model(bert_layer,max_len=512):\n    input_word_ids = Input(shape=(max_len),dtype=tf.int32,name=\"input_word_ids\") # name은 반드시 고정!\n    input_mask = Input(shape=(max_len),dtype=tf.int32,name=\"input_mask\")\n    input_segment_ids = Input(shape=(max_len),dtype=tf.int32,name=\"segment_ids\")\n    _,sequence_output = bert_layer([input_word_ids,input_mask,input_segment_ids]) \n    clf_output = sequence_output[:,0,:]  \n    # :(데이터 batch--> 모든 데이터 처리) , 0(데이터셋의 모든 단어를 사용하겠다는 뜻) , :(각 단어 임베딩)\n    output = Dense(2,activation='softmax')(clf_output)\n    model = Model(inputs=[input_word_ids,input_mask,input_segment_ids],outputs=output)\n    model.compile(loss='categorical_crossentropy',metrics=['acc'],optimizer=Adam(lr=0.00002))\n    return model\n\nmodel = build_model(bert_layer,max_len=110)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T06:14:45.907291Z","iopub.execute_input":"2021-12-08T06:14:45.90806Z","iopub.status.idle":"2021-12-08T06:14:47.208117Z","shell.execute_reply.started":"2021-12-08T06:14:45.908015Z","shell.execute_reply":"2021-12-08T06:14:47.207345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델 학습\n\ncp = callbacks.ModelCheckpoint('best-bert-model.h5')\nes = callbacks.EarlyStopping(patience=3,restore_best_weights=True,verbose=1)\nrl = callbacks.ReduceLROnPlateau(patience=2)\nhistory_2 = model.fit(train_input,pd.get_dummies(target),batch_size=32,epochs=5,validation_split=0.2,callbacks=[cp,es,rl])","metadata":{"execution":{"iopub.status.busy":"2021-12-08T06:14:54.422303Z","iopub.execute_input":"2021-12-08T06:14:54.422982Z","iopub.status.idle":"2021-12-08T06:57:48.999557Z","shell.execute_reply.started":"2021-12-08T06:14:54.422942Z","shell.execute_reply":"2021-12-08T06:57:48.99776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 채점\n#### train 데이터와 동일한 전처리 실행\n#### 위의 bert_encode 함수에 데이터 삽입","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('test_data.csv')\ninput_2 = test['html']\ntest_input = bert_encode(input_2.values,tk,max_len=110)  # max_len은 변경가능!","metadata":{"execution":{"iopub.status.busy":"2021-12-08T07:04:17.961611Z","iopub.execute_input":"2021-12-08T07:04:17.961892Z","iopub.status.idle":"2021-12-08T07:05:32.453766Z","shell.execute_reply.started":"2021-12-08T07:04:17.96186Z","shell.execute_reply":"2021-12-08T07:05:32.453031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 위 학습에서 가장 좋은 결과가 나온 옵션 로드\nbert_model = models.load_model('best-bert-model.h5',custom_objects={'KerasLayer':hub.KerasLayer})\nbert_model.evaluate(test_input,pd.get_dummies(test['label']))","metadata":{"execution":{"iopub.status.busy":"2021-12-08T07:08:43.438822Z","iopub.execute_input":"2021-12-08T07:08:43.439071Z","iopub.status.idle":"2021-12-08T07:10:23.999386Z","shell.execute_reply.started":"2021-12-08T07:08:43.439041Z","shell.execute_reply":"2021-12-08T07:10:23.998681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = bert_model.predict(test_input)\nprint(result)\nprint('---------------------------------------정답 확인---------------------------------------')\nprint(result.argmax(1))","metadata":{"execution":{"iopub.status.busy":"2021-12-08T07:10:24.001062Z","iopub.execute_input":"2021-12-08T07:10:24.001412Z","iopub.status.idle":"2021-12-08T07:10:47.620263Z","shell.execute_reply.started":"2021-12-08T07:10:24.001373Z","shell.execute_reply":"2021-12-08T07:10:47.619491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_res = pd.DataFrame(result.argmax(1))\ndf_res.to_csv('bert_check.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T07:11:17.254156Z","iopub.execute_input":"2021-12-08T07:11:17.255023Z","iopub.status.idle":"2021-12-08T07:11:17.271376Z","shell.execute_reply.started":"2021-12-08T07:11:17.254979Z","shell.execute_reply":"2021-12-08T07:11:17.270515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 결론\n#### 장점 \n* 비교적 짧은 시간내 정확한 학습 가능\n* 모델을 거의 그대로 가져오기 때문에 복잡하게 층을 여러개 쌓을필요 X\n* 버리는 단어 없이 거의 모든 단어 학습가능\n#### 단점\n* 학습할 html 길이를 110보다 높게 설정시 RAM 터지는 현상발생\n* max_len 높일시(최소 500은 필요) 학습시간 예상 불가능\n","metadata":{}}]}